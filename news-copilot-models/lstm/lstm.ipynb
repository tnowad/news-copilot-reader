{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "451bf45b-6edf-46ec-a300-d2b513fa93c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.venv/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from torch) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.11/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.11/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.venv/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.venv/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.venv/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.venv/lib/python3.11/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.venv/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.venv/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.venv/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.venv/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.venv/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./.venv/lib/python3.11/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.venv/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in ./.venv/lib/python3.11/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.venv/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff71d261-022f-4bef-adba-6a75d6f726b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3575717c-5aa6-443c-a7fc-57ead02957af",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpus.txt', 'r') as file:\n",
    "    data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f1600aa-0173-453c-ba3d-640441d27af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = data.split()\n",
    "word_to_ix = {word: i for i, word in enumerate(set(words))}\n",
    "ix_to_word = {i: word for word, i in word_to_ix.items()}\n",
    "vocab_size = len(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cea06f4a-7ac0-48e8-a78f-d22a331d69e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_idx = [word_to_ix[word] for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b526814b-e485-4403-b446-58c1bb2ecfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 10\n",
    "sequences = []\n",
    "for i in range(len(data_idx) - seq_length):\n",
    "    sequences.append(data_idx[i:i+seq_length+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc9801a6-ebcc-4f7a-9399-c873f33d62a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.array(sequences)\n",
    "X = torch.from_numpy(sequences[:, :-1])\n",
    "y = torch.from_numpy(sequences[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46c42657-744a-423d-a48d-35f0c499880d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "053a1369-61a8-46ef-8584-8f662839c631",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddd87287-1150-4f2b-b37b-b83ecf6bb873",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7bac150-4442-4c6d-8b7f-aff2747ad128",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "hidden_dim = 150\n",
    "num_layers = 2\n",
    "model = LSTMModel(vocab_size, embedding_dim, hidden_dim, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46f4b1d2-06c5-4729-b3f4-8cb62001fcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "def generate_text(seed_text, next_words, model, word_to_ix, ix_to_word, temperature=1.0):\n",
    "    generated_text = seed_text\n",
    "    for _ in range(next_words):\n",
    "        seed_tokens = seed_text.split()\n",
    "        if len(seed_tokens) < seq_length:\n",
    "            pad_length = seq_length - len(seed_tokens)\n",
    "            seed_tokens = ['<pad>'] * pad_length + seed_tokens\n",
    "        seed_idx = torch.tensor([[word_to_ix.get(word, 0) for word in seed_tokens]])\n",
    "        with torch.no_grad():\n",
    "            output = model(seed_idx)\n",
    "        \n",
    "        output_dist = output.squeeze().div(temperature).exp()\n",
    "        word_idx = torch.multinomial(output_dist, 1).item()\n",
    "        \n",
    "        predicted_word = ix_to_word.get(word_idx, '<unk>')\n",
    "        generated_text += \" \" + predicted_word\n",
    "        seed_text = ' '.join(seed_text.split()[1:]) + \" \" + predicted_word\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa4978c5-65ec-44bb-816b-4b4b2e946299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Người dân Quỳnh Đôi không đồng tình ghép tên với xã hội Việt Nam Đây là ra xin trước khi xe không có xác định sang quá thẩm, toàn từ camera\n",
      "Epoch [1/100], Loss: 4.6082\n",
      "Người dân Quỳnh Đôi không đồng tình ghép tên với xã mang Italy ý thức mặc dù chôn trang cứ năm Hoàng Cục được cấp địa phương đều còn lại là\n",
      "Epoch [2/100], Loss: 5.8856\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m----> 4\u001b[0m         \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m      6\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n",
      "File \u001b[0;32m~/Documents/dataCode/python/model_stacked_lstm/.venv/lib/python3.11/site-packages/torch/_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/dataCode/python/model_stacked_lstm/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:480\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m prior \u001b[38;5;241m=\u001b[39m set_eval_frame(callback)\n\u001b[1;32m    479\u001b[0m backend_cache_manager \u001b[38;5;241m=\u001b[39m backend_cache_wrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback)\n\u001b[0;32m--> 480\u001b[0m \u001b[43mbackend_cache_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__enter__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m backend_ctx \u001b[38;5;241m=\u001b[39m backend_ctx_ctor()\n\u001b[1;32m    482\u001b[0m backend_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n",
      "File \u001b[0;32m/usr/lib/python3.11/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/dataCode/python/model_stacked_lstm/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:122\u001b[0m, in \u001b[0;36mbackend_cache_wrapper\u001b[0;34m(callback)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackend_cache_wrapper\u001b[39m(callback: DynamoCallback):\n\u001b[0;32m--> 122\u001b[0m     \u001b[43m_maybe_init_guarded_backend_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;66;03m# callback is False for RunOnlyContext. RunOnlyContext is used\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;66;03m# as a way to re-use the previous compiled cache.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;66;03m# We therefore skip the check and re-use whatever code that's already cached.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;66;03m# Note: the cache that's actually used depends on the caching policy.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/dataCode/python/model_stacked_lstm/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:100\u001b[0m, in \u001b[0;36m_maybe_init_guarded_backend_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_init_guarded_backend_cache\u001b[39m():\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(guarded_backend_cache, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_backend_check_for_run_only_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    101\u001b[0m         guarded_backend_cache\u001b[38;5;241m.\u001b[39mskip_backend_check_for_run_only_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(guarded_backend_cache, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    seed_text = \"Người dân Quỳnh Đôi không đồng tình ghép tên với xã\"\n",
    "    generated_text = generate_text(seed_text, 20, model, word_to_ix, ix_to_word, temperature=1)\n",
    "    print(generated_text)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ca6547-4a2f-4bad-991e-d1f67d86caad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, model, word_to_ix, ix_to_word, temperature=1.0):\n",
    "    generated_text = seed_text\n",
    "    for _ in range(next_words):\n",
    "        seed_tokens = seed_text.split()\n",
    "        if len(seed_tokens) < seq_length:\n",
    "            pad_length = seq_length - len(seed_tokens)\n",
    "            seed_tokens = ['<pad>'] * pad_length + seed_tokens\n",
    "        seed_idx = torch.tensor([[word_to_ix.get(word, 0) for word in seed_tokens]])\n",
    "        with torch.no_grad():\n",
    "            output = model(seed_idx)\n",
    "        \n",
    "        output_dist = output.squeeze().div(temperature).exp()\n",
    "        word_idx = torch.multinomial(output_dist, 1).item()\n",
    "        \n",
    "        predicted_word = ix_to_word.get(word_idx, '<unk>')\n",
    "        generated_text += \" \" + predicted_word\n",
    "        seed_text = ' '.join(seed_text.split()[1:]) + \" \" + predicted_word\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92af693d-43ef-462d-853b-8d247ef53c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Chưa bên 1 triệu đồng đối với nhiệm vụ án đầu Quảng Ninh Bình Dương, cuộc họp bình nhất, hành\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"AI\"\n",
    "generated_text = generate_text(seed_text, 20, model, word_to_ix, ix_to_word, temperature=1)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2252a6d4-ad58-44c0-988e-35b8afdbdfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"lstm_model.pth\"\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce52459c-2702-498b-bb5f-2a4fb3602c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = LSTMModel(vocab_size, embedding_dim, hidden_dim, num_layers)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    " q\n",
    "print(\"Model loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ba1e19-ef5e-47ea-8174-1148b5102674",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
